import logging\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Union, List\n\nimport fasttext\nfrom robust_downloader import download\n\nlogger = logging.getLogger(__name__)\nMODELS = {"low_mem": None, "high_mem": None}\nFTLANG_CACHE = os.getenv("FTLANG_CACHE", "/tmp/fasttext-langdetect")\n\n\nclass DetectError(Exception):\n    """Custom exception for detection errors."""\n    pass\n\n\ndef get_model_map(low_memory: bool = False) -> tuple:\n    """\n    Returns the model map based on the low_memory flag.\n    :param low_memory: Boolean flag to determine the model size.\n    :return: Tuple containing mode, cache path, model name, and download URL.\n    """\n    if low_memory:\n        return "low_mem", FTLANG_CACHE, "lid.176.ftz", "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"\n    else:\n        return "high_mem", FTLANG_CACHE, "lid.176.bin", "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"\n\n\ndef get_model_loaded(low_memory: bool = False, download_proxy: str = None) -> fasttext.FastText._FastText:  # type: ignore\n    """\n    Loads the FastText model based on the low_memory flag.\n    :param low_memory: Boolean flag to determine the model size.\n    :param download_proxy: Proxy URL for downloading the model.\n    :return: Loaded FastText model.\n    :raises DetectError: If the model cannot be loaded or downloaded.\n    """\n    mode, cache, name, url = get_model_map(low_memory)\n    loaded = MODELS.get(mode, None)\n    if loaded:\n        return loaded\n    model_path = os.path.join(cache, name)\n    if Path(model_path).exists():\n        if Path(model_path).is_dir():\n            raise DetectError(f"{model_path} is a directory")\n        try:\n            loaded_model = fasttext.load_model(model_path)\n            MODELS[mode] = loaded_model\n        except Exception as e:\n            logger.error(f"Error loading model {model_path}: {e}")\n            download(url=url, folder=cache, filename=name, proxy=download_proxy)\n            raise DetectError(f"Failed to load model: {e}")\n        else:\n            return loaded_model\n\n    download(url=url, folder=cache, filename=name, proxy=download_proxy, retry_max=3, timeout=20)\n    loaded_model = fasttext.load_model(model_path)\n    MODELS[mode] = loaded_model\n    return loaded_model\n\n\ndef detect(text: str, *, low_memory: bool = True, model_download_proxy: str = None) -> Dict[str, Union[str, float]]:\n    """\n    Detects the language of the given text.\n    :param text: The text to detect the language of.\n    :param low_memory: Boolean flag to determine the model size.\n    :param model_download_proxy: Proxy URL for downloading the model.\n    :return: Dictionary containing the detected language and confidence score.\n    :raises DetectError: If the detection fails.\n    """\n    try:\n        model = get_model_loaded(low_memory=low_memory, download_proxy=model_download_proxy)\n        labels, scores = model.predict(text)\n        label = labels[0].replace("__label__", '')\n        score = min(float(scores[0]), 1.0)\n        return {"lang": label, "score": score}\n    except Exception as e:\n        raise DetectError(f"Detection failed: {e}")\n\n\ndef detect_multilingual(text: str, *, low_memory: bool = True, model_download_proxy: str = None, k: int = 5, threshold: float = 0.0, on_unicode_error: str = "strict") -> List[Dict[str, Union[str, float]]]:\n    """\n    Detects multiple languages in the given text.\n    :param text: The text to detect languages in.\n    :param low_memory: Boolean flag to determine the model size.\n    :param model_download_proxy: Proxy URL for downloading the model.\n    :param k: Number of top predictions to return.\n    :param threshold: Confidence score threshold for predictions.\n    :param on_unicode_error: Error handling strategy for Unicode errors.\n    :return: List of dictionaries containing detected languages and confidence scores.\n    :raises DetectError: If the detection fails.\n    """\n    try:\n        model = get_model_loaded(low_memory=low_memory, download_proxy=model_download_proxy)\n        labels, scores = model.predict(text=text, k=k, threshold=threshold, on_unicode_error=on_unicode_error)\n        detect_result = []\n        for label, score in zip(labels, scores):\n            label = label.replace("__label__", '')\n            score = min(float(score), 1.0)\n            detect_result.append({"lang": label, "score": score})\n        return sorted(detect_result, key=lambda i: i[\'score\'], reverse=True)\n    except Exception as e:\n        raise DetectError(f"Multilingual detection failed: {e}")\n