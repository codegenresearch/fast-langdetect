# -*- coding: utf-8 -*-\n# @Time    : 2024/1/17 下午8:30\n# @Author  : sudoskys\n# @File    : infer.py\n# @Software: PyCharm\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Union, List, Optional\nimport fasttext\nfrom robust_downloader import download\nlogger = logging.getLogger(__name__)\nMODELS = {"low_mem": None, "high_mem": None}\nFTLANG_CACHE = os.getenv("FTLANG_CACHE", "/tmp/fasttext-langdetect")\n__all__ = ["detect", "detect_multilingual"]\n\ntry:\n    # silences warnings as the package does not properly use the python 'warnings' package\n    # see https://github.com/facebookresearch/fastText/issues/1056\n    fasttext.FastText.eprint = lambda *args, **kwargs: None\nexcept Exception:\n    pass\nclass DetectError(Exception):\n    """Custom exception for detection errors."""\n    pass\ndef get_model_map(low_memory: bool = False) -> tuple:\n    """\n    Get the model map based on the low_memory flag.\n    ":param low_memory: Boolean flag to indicate if low memory model should be used.":return: Tuple containing mode, cache path, model name, and model URL.":rtype: tuple"""\n    if low_memory:\n        return "low_mem", FTLANG_CACHE, "lid.176.ftz", "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"\n    else:\n        return "high_mem", FTLANG_CACHE, "lid.176.bin", "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"\ndef get_model_loaded(\n        low_memory: bool = False,\n        download_proxy: Optional[str] = None,\n) -> fasttext.FastText._FastText:\n    """\n    Load the appropriate model based on the low_memory flag.\n    ":param low_memory: Boolean flag to indicate if low memory model should be used.":param download_proxy: Proxy for downloading the model.":return: Loaded fastText model.":rtype: fasttext.FastText._FastText"""\n    mode, cache, name, url = get_model_map(low_memory)\n    loaded = MODELS.get(mode, None)\n    if loaded:\n        return loaded\n    model_path = os.path.join(cache, name)\n    if Path(model_path).exists():\n        if Path(model_path).is_dir():\n            raise Exception(f"{model_path} is a directory")\n        try:\n            loaded_model = fasttext.load_model(model_path)\n            MODELS[mode] = loaded_model\n        except Exception as e:\n            logger.error(f"Error loading model {model_path}: {e}")\n            download(url=url, folder=cache, filename=name, proxy=download_proxy)\n            raise e from None\n        else:\n            return loaded_model\n    download(url=url, folder=cache, filename=name, proxy=download_proxy, retry_max=3, timeout=20)\n    loaded_model = fasttext.load_model(model_path)\n    MODELS[mode] = loaded_model\n    return loaded_model\ndef detect(\n        text: str, *,\n        low_memory: bool = True,\n        model_download_proxy: Optional[str] = None,\n) -> Dict[str, Union[str, float]]:\n    """\n    Detect the language of the given text.\n    ":param text: The text to detect the language of.":param low_memory: Boolean flag to indicate if low memory model should be used.":param model_download_proxy: Proxy for downloading the model.":return: Dictionary containing the detected language and score.":rtype: Dict[str, Union[str, float]]"""\n    if not text:\n        raise ValueError("Input text cannot be empty")\n    try:\n        model = get_model_loaded(low_memory=low_memory, download_proxy=model_download_proxy)\n        labels, scores = model.predict(text)\n        label = labels[0].replace("__label__", '')\n        score = min(float(scores[0]), 1.0)\n        return {\